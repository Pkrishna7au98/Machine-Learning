{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebaac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "class LogisticRegression(object):    \n",
    "    \"\"\"LogisticRegression \n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    learningRate : float, optional\n",
    "        Constant by which updates are multiplied (falls between 0 and 1). Defaults to 0.01.\n",
    "     \n",
    "    numIterations : int, optional\n",
    "        Number of passes (or epochs) over the training data. Defaults to 10.\n",
    "    \n",
    "    penalty : None or 'L2'\n",
    "        Option to perform L2 regularization. Defaults to None.\n",
    "    \n",
    "    Attributes\n",
    "    ------------\n",
    "    weights : 1d-array, shape = [1, 1 + n_features]\n",
    "        Weights after training phase\n",
    "        \n",
    "    iterationsPerformed : int\n",
    "        Number of iterations of gradient descent performed prior to hitting tolerance level\n",
    "        \n",
    "    costs : list, length = numIterations\n",
    "        Value of the log-likelihood cost function for each iteration of gradient descent\n",
    "        \n",
    "    References\n",
    "    ------------\n",
    "    https://en.wikipedia.org/wiki/Logistic_regression\n",
    "    https://en.wikipedia.org/wiki/Regularization_(mathematics)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learningRate, numIterations = 10, penalty = None, C = 0.01):\n",
    "        \n",
    "        self.learningRate = learningRate\n",
    "        self.numIterations = numIterations\n",
    "        self.penalty = penalty\n",
    "        self.C = C\n",
    "        \n",
    "    def train(self, X_train, y_train, tol = 10 ** -4):\n",
    "        \"\"\"Fit weights to the training data\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X_train : {array-like}, shape = [n_samples, n_features]\n",
    "            Training data to be fitted, where n_samples is the number of \n",
    "            samples and n_features is the number of features. \n",
    "            \n",
    "        y_train : array-like, shape = [n_samples,], values = 1|0\n",
    "            Labels (target values). \n",
    "        tol : float, optional\n",
    "            Value indicating the weight change between epochs in which\n",
    "            gradient descent should terminated. Defaults to 10 ** -4\n",
    "        Returns:\n",
    "        -----------\n",
    "        self : object\n",
    "        \n",
    "        \"\"\"\n",
    "        tolerance = tol * np.ones([1, np.shape(X_train)[1] + 1])\n",
    "        self.weights = np.zeros(np.shape(X_train)[1] + 1) \n",
    "        X_train = np.c_[np.ones([np.shape(X_train)[0], 1]), X_train]\n",
    "        self.costs = []\n",
    "\n",
    "        for i in range(self.numIterations):\n",
    "            \n",
    "            z = np.dot(X_train, self.weights)\n",
    "            errors = y_train - logistic_func(z)\n",
    "            if self.penalty is not None:                \n",
    "                delta_w = self.learningRate * (self.C * np.dot(errors, X_train) + np.sum(self.weights))  \n",
    "            else:\n",
    "                delta_w = self.learningRate * np.dot(errors, X_train)\n",
    "                \n",
    "            self.iterationsPerformed = i\n",
    "\n",
    "            if np.all(abs(delta_w) >= tolerance): \n",
    "                #weight update\n",
    "                self.weights += delta_w                                \n",
    "                #Costs\n",
    "                if self.penalty is not None:\n",
    "                    self.costs.append(reg_logLiklihood(X_train, self.weights, y_train, self.C))\n",
    "                else:\n",
    "                    self.costs.append(logLiklihood(z, y_train))\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        return self\n",
    "                    \n",
    "    def predict(self, X_test, pi = 0.5):\n",
    "        \"\"\"predict class label \n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X_test : {array-like}, shape = [n_samples, n_features]\n",
    "            Testing data, where n_samples is the number of samples\n",
    "            and n_features is the number of features. n_features must\n",
    "            be equal to the number of features in X_train.\n",
    "            \n",
    "        pi : float, cut-off probability, optional\n",
    "            Probability threshold for predicting positive class. Defaults to 0.5.\n",
    "        \n",
    "        Returns\n",
    "        ------------\n",
    "        predictions : list, shape = [n_samples,], values = 1|0\n",
    "            Class label predictions based on the weights fitted following \n",
    "            training phase.\n",
    "        \n",
    "        probs : list, shape = [n_samples,]\n",
    "            Probability that the predicted class label is a member of the \n",
    "            positive class (falls between 0 and 1).\n",
    "        \n",
    "        \"\"\"        \n",
    "        z = self.weights[0] + np.dot(X_test, self.weights[1:])        \n",
    "        probs = np.array([logistic_func(i) for i in z])\n",
    "        predictions = np.where(probs >= pi, 1, 0)\n",
    "       \n",
    "        return predictions, probs\n",
    "        \n",
    "    def performanceEval(self, predictions, y_test):\n",
    "        \"\"\"Computer binary classification performance metrics\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        predictions : list, shape = [n_samples,], values = 1|0\n",
    "            Class label predictions based on the weights fitted following \n",
    "            training phase. \n",
    "        \n",
    "        y_test : list, shape = [n_samples,], values = 1|0\n",
    "            True class labels\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        performance : dict\n",
    "            Accuracy, sensitivity (recall), specificity, postitive predictive \n",
    "            value (PPV, precision), negative predictive value (NPV), false \n",
    "            negative rate (FNR, \"miss rate\"), false positive rate (FPR, \"fall out\")\n",
    "        \"\"\"        \n",
    "        #Initialize\n",
    "        TP, TN, FP, FN, P, N = 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "        for idx, test_sample in enumerate(y_test):\n",
    "            \n",
    "            if predictions[idx] == 1 and test_sample == 1:\n",
    "                TP += 1       \n",
    "                P += 1\n",
    "            elif predictions[idx] == 0 and test_sample == 0:                \n",
    "                TN += 1\n",
    "                N += 1\n",
    "            elif predictions[idx] == 0 and test_sample == 1:\n",
    "                FN += 1\n",
    "                P += 1\n",
    "            elif predictions[idx] == 1 and test_sample == 0:\n",
    "                FP += 1\n",
    "                N += 1\n",
    "            \n",
    "        accuracy = (TP + TN) / (P + N)                \n",
    "        sensitivity = TP / P        \n",
    "        specificity = TN / N        \n",
    "        PPV = TP / (TP + FP)        \n",
    "        NPV = TN / (TN + FN)        \n",
    "        FNR = 1 - sensitivity        \n",
    "        FPR = 1 - specificity\n",
    "        \n",
    "        performance = {'Accuracy': accuracy, 'Sensitivity': sensitivity,\n",
    "                       'Specificity': specificity, 'Precision': PPV,\n",
    "                       'NPV': NPV, 'FNR': FNR, 'FPR': FPR}        \n",
    "      \n",
    "        return performance\n",
    "        \n",
    "    def predictionPlot(self, X_test, y_test):\n",
    "        \"\"\"Plot of test samples mapped onto the logistic function (sigmoidal\n",
    "        curve) according to the fitted weights. \n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X_test : {array-like}, shape = [n_samples, n_features]\n",
    "            Testing data, where n_samples is the number of samples\n",
    "            and n_features is the number of features. n_features must\n",
    "            be equal to the number of features in X_train.\n",
    "        \n",
    "        y_test : list, shape = [n_samples,], values = 1|0\n",
    "            True class labels\n",
    "        \n",
    "        Returns\n",
    "        -----------       \n",
    "        matploblib figure\n",
    "        \n",
    "        \"\"\"\n",
    "        zs = self.weights[0] + np.dot(X_test, self.weights[1:])        \n",
    "        probs = np.array([logistic_func(i) for i in zs])\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(-10, 10, 0.1), logistic_func(np.arange(-10, 10, 0.1)))        \n",
    "        colors = ['r','b']\n",
    "        probs = np.array(probs)\n",
    "        for idx,cl in enumerate(np.unique(y_test)):\n",
    "            plt.scatter(x = zs[np.where(y_test == cl)[0]], y = probs[np.where(y_test == cl)[0]],\n",
    "                    alpha = 0.8, c = colors[idx],\n",
    "                    marker = 'o', label = cl, s = 30)\n",
    "\n",
    "        plt.xlabel('z')\n",
    "        plt.ylim([-0.1, 1.1])\n",
    "        plt.axhline(0.0, ls = 'dotted', color = 'k')\n",
    "        plt.axhline(1.0, ls = 'dotted', color = 'k')\n",
    "        plt.axvline(0.0, ls = 'dotted', color = 'k')\n",
    "        plt.ylabel('$\\phi (z)$')\n",
    "        plt.legend(loc = 'upper left')\n",
    "        plt.title('Logistic Regression Prediction Curve')\n",
    "        plt.show()\n",
    "        \n",
    "    def plotCost(self):\n",
    "        \"\"\"Plot value of log-liklihood cost function for each epoch\n",
    "                \n",
    "        Returns\n",
    "        --------\n",
    "        matplotlib figure       \n",
    "        \n",
    "        \"\"\"        \n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(1, self.iterationsPerformed + 1), self.costs, marker = '.')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Log-Liklihood J(w)')\n",
    "                \n",
    "    def plotDecisionRegions(self, X_test, y_test, pi = 0.5, res = 0.01):\n",
    "        \"\"\"Visualize decision boundaries of trained logistic regression\n",
    "        classifier. Note, this method can only be used when training classifier\n",
    "        with 2 features.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X_test : {array-like}, shape = [n_samples, n_features]\n",
    "            Testing data, where n_samples is the number of samples\n",
    "            and n_features is the number of features. n_features must\n",
    "            be equal to the number of features in X_train.\n",
    "        y_test : list, shape = [n_samples,], values = 1|0\n",
    "            True class labels\n",
    "            \n",
    "        pi : float, cut-off probability, optional\n",
    "            Probability threshold for predicting positive class. Defaults to 0.5.\n",
    "            \n",
    "        res : float\n",
    "            Resolution of contour grid\n",
    "        Returns\n",
    "        -----------\n",
    "        matplotlib figure        \n",
    "        \n",
    "        \"\"\"        \n",
    "        x = np.arange(min(X_test[:,0]) - 1, max(X_test[:,0]) + 1, 0.01)\n",
    "        y = np.arange(min(X_test[:,1]) - 1, max(X_test[:,1]) + 1, 0.01)        \n",
    "        xx, yy = np.meshgrid(x, y, indexing = 'xy')\n",
    "        \n",
    "        data_points = np.transpose([xx.ravel(), yy.ravel()])\n",
    "        preds, probs = self.predict(data_points, pi)\n",
    "            \n",
    "        colors = ['r','b']        \n",
    "        probs = np.array(probs)\n",
    "                \n",
    "        for idx,cl in enumerate(np.unique(y_test)):\n",
    "            plt.scatter(x = X_test[:,0][np.where(y_test == cl)[0]], y = X_test[:,1][np.where(y_test == cl)[0]],\n",
    "                    alpha = 0.8, c = colors[idx],\n",
    "                    marker = 'o', label = cl, s = 30)\n",
    "                    \n",
    "        preds = preds.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, preds, alpha = 0.3)\n",
    "        plt.legend(loc = 'best')\n",
    "        plt.xlabel('$x_1$', size = 'x-large')\n",
    "        plt.ylabel('$x_2$', size = 'x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff06893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(z):   \n",
    "    \"\"\"Logistic (sigmoid) function, inverse of logit function\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    z : float\n",
    "        linear combinations of weights and sample features\n",
    "        z = w_0 + w_1*x_1 + ... + w_n*x_n\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    Value of logistic function at z\n",
    "    \n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2946cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLiklihood(z, y):\n",
    "    \"\"\"Log-liklihood function (cost function to be minimized in logistic\n",
    "    regression classification)\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    z : float\n",
    "        linear combinations of weights and sample features\n",
    "        z = w_0 + w_1*x_1 + ... + w_n*x_n\n",
    "        \n",
    "    y : list, values = 1|0\n",
    "        target values\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    Value of log-liklihood function with parameter z and target value y\n",
    "    \"\"\"\n",
    "    return -1 * np.sum((y * np.log(logistic_func(z))) + ((1 - y) * np.log(1 - logistic_func(z))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b67c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logLiklihood(x, weights, y, C):\n",
    "    \"\"\"Regularizd log-liklihood function (cost function to minimized in logistic\n",
    "    regression classification with L2 regularization)\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x : {array-like}, shape = [n_samples, n_features + 1]\n",
    "        feature vectors. Note, first column of x must be\n",
    "        a vector of ones.\n",
    "    \n",
    "    weights : 1d-array, shape = [1, 1 + n_features]\n",
    "        Coefficients that weight each samples feature vector\n",
    "        \n",
    "    y : list, shape = [n_samples,], values = 1|0\n",
    "        target values\n",
    "        \n",
    "    C : float\n",
    "        Regularization parameter. C is equal to 1/lambda    \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    Value of regularized log-liklihood function with the given feature values,\n",
    "    weights, target values, and regularization parameter\n",
    "     \n",
    "    \"\"\"\n",
    "    z = np.dot(x, weights) \n",
    "    reg_term = (1 / (2 * C)) * np.dot(weights.T, weights)\n",
    "    \n",
    "    return -1 * np.sum((y * np.log(logistic_func(z))) + ((1 - y) * np.log(1 - logistic_func(z)))) + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749fc8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
